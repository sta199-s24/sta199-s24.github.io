{
  "hash": "cf2287c223c255ffffb5570beaec5763",
  "result": {
    "engine": "knitr",
    "markdown": "---\ntitle: \"Building a spam filter\"\ncategories: \n  - Application exercise\n  - Answers\neditor: visual\neditor_options: \n  chunk_output_type: console\n---\n\n\n\nIn this application exercise, we will\n\n-   Use logistic regression to fit a model for a binary response variable\n-   Fit a logistic regression model in R\n-   Use a logistic regression model for classification\n\nTo illustrate logistic regression, we will build a spam filter from email data.\n\nThe data come from incoming emails in David Diez's (one of the authors of OpenIntro textbooks) Gmail account for the first three months of 2012.\nAll personally identifiable information has been removed.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(openintro)\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nglimpse(email)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nRows: 3,921\nColumns: 21\n$ spam         <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ to_multiple  <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ from         <fct> 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, …\n$ cc           <int> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 1, 2, 1, 0, 2, 0, …\n$ sent_email   <fct> 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, …\n$ time         <dttm> 2012-01-01 01:16:41, 2012-01-01 02:03:59, 2012-01-01 11:…\n$ image        <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ attach       <dbl> 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ dollar       <dbl> 0, 0, 4, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 5, 0, 0, …\n$ winner       <fct> no, no, no, no, no, no, no, no, no, no, no, no, no, no, n…\n$ inherit      <dbl> 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ viagra       <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ password     <dbl> 0, 0, 0, 0, 2, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ num_char     <dbl> 11.370, 10.504, 7.773, 13.256, 1.231, 1.091, 4.837, 7.421…\n$ line_breaks  <int> 202, 202, 192, 255, 29, 25, 193, 237, 69, 68, 25, 79, 191…\n$ format       <fct> 1, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, …\n$ re_subj      <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, …\n$ exclaim_subj <dbl> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, …\n$ urgent_subj  <fct> 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, …\n$ exclaim_mess <dbl> 0, 1, 6, 48, 1, 1, 1, 18, 1, 0, 2, 1, 0, 10, 4, 10, 20, 0…\n$ number       <fct> big, small, small, small, none, none, big, small, small, …\n```\n\n\n:::\n:::\n\n\n\nThe variables we'll use in this analysis are\n\n-   `spam`: 1 if the email is spam, 0 otherwise\n-   `exclaim_mess`: The number of exclamation points in the email message\n\n**Goal:** Use the number of exclamation points in an email to predict whether or not it is spam.\n\n# Exploratory data analysis\n\nLet's start by taking a look at our data.\nCreate an density plot to investigate the relationship between `spam` and `exclaim_mess`.\nAdditionally, calculate the mean number of exclamation points for both spam and non-spam emails.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(email, aes(x = exclaim_mess, fill = spam)) + \n  geom_density()\n```\n\n::: {.cell-output-display}\n![](ae-14-spam-filter-A_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n\n```{.r .cell-code}\nemail |>\n  group_by(spam) |>\n  summarize(mean_ep = mean(exclaim_mess))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 2\n  spam  mean_ep\n  <fct>   <dbl>\n1 0        6.51\n2 1        7.32\n```\n\n\n:::\n:::\n\n\n\n# Linear model -- a false start\n\nSuppose we try using a linear model to describe the relationship between the number of exclamation points and whether an email is spam.\nWrite up a linear model that models spam by exclamation marks.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlinear_reg() |>\n  fit(as.numeric(spam) ~ exclaim_mess, data = email)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\nparsnip model object\n\n\nCall:\nstats::lm(formula = as.numeric(spam) ~ exclaim_mess, data = data)\n\nCoefficients:\n (Intercept)  exclaim_mess  \n   1.093e+00     2.604e-05  \n```\n\n\n:::\n:::\n\n\n\nA visualization of a linear model is below.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(email, aes(x = exclaim_mess, y = as.numeric(spam), color = spam)) + \n  geom_jitter(alpha = 0.5) + \n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\")\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\n`geom_smooth()` using formula = 'y ~ x'\n```\n\n\n:::\n\n::: {.cell-output-display}\n![](ae-14-spam-filter-A_files/figure-html/plot-linear-model-1.png){width=672}\n:::\n:::\n\n\n\n-   **Your turn:** Is the linear model a good fit for the data? Why or why not?\n\n*No.*\n\n# Logistic regression -- a different approach\n\nLet $p$ be the probability an email is spam (success).\n\n-   $\\frac{p}{1-p}$: odds an email is spam (if p = 0.7, then the odds are 0.7/(1 - 0.7) = 2.33)\n-   $\\log\\Big(\\frac{p}{1-p}\\Big)$: \"log-odds\", i.e., the natural log, an email is spam\n\nThe logistic regression model using the number of exclamation points as an explanatory variable is as follows:\n\n$$\\log\\Big(\\frac{p}{1-p}\\Big) = \\beta_0 + \\beta_1 \\times exclaim\\_mess$$\n\nThe probability an email is spam can be calculated as:\n\n$$p = \\frac{\\exp\\{\\beta_0 + \\beta_1 \\times exclaim\\_mess\\}}{1 + \\exp\\{\\beta_0 + \\beta_1 \\times exclaim\\_mess\\}}$$\n\n# Exercises\n\n## Exercise 1\n\n-   **Demo:** Fit the logistic regression model using the number of exclamation points to predict the probability an email is spam.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_fit <- logistic_reg() |>\n  fit(spam ~ exclaim_mess, data = email)\n\ntidy(log_fit)\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 2 × 5\n  term          estimate std.error statistic p.value\n  <chr>            <dbl>     <dbl>     <dbl>   <dbl>\n1 (Intercept)  -2.27      0.0553     -41.1     0    \n2 exclaim_mess  0.000272  0.000949     0.287   0.774\n```\n\n\n:::\n:::\n\n\n\n-   **Your turn:** How does the code above differ from previous code we've used to fit regression models? Compare your summary output to the estimated model below.\n\n$$\\log\\Big(\\frac{p}{1-p}\\Big) = -2.27 - 0.000272 \\times exclaim\\_mess$$\n\nWe use logistic instead of linear regression.\n\n## Exercise 2\n\nWhat is the probability the email is spam if it contains 10 exclamation points?\nAnswer the question using the `predict()` function.\n\nWe can use the predict function in R to produce the probability as well.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nep_10 <- tibble(exclaim_mess = 10)\npredict(log_fit, ep_10, type = \"prob\")\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 1 × 2\n  .pred_0 .pred_1\n    <dbl>   <dbl>\n1   0.906  0.0937\n```\n\n\n:::\n:::\n\n\n\n## Exercise 3\n\nWe have the probability an email is spam, but ultimately we want to use the probability to classify an email as spam or not spam.\nTherefore, we need to set a **decision-making threshold**, such that an email is classified as spam if the predicted probability is greater than the threshold and not spam otherwise.\n\nSuppose you are a data scientist working on a spam filter.\nYou must determine how high the predicted probability must be before you think it would be reasonable to call it spam and put it in the junk folder (which the user is unlikely to check).\n\n**Your turn:** What are some trade offs you would consider as you set the decision-making threshold?\nDiscuss with your neighbor.\n\n*Answers will vary.*\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\naugment(log_fit, email) |>\n  select(spam, exclaim_mess, .pred_class) |>\n  ggplot(aes(x = exclaim_mess, y = spam, color = .pred_class)) +\n  geom_jitter(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](ae-14-spam-filter-A_files/figure-html/unnamed-chunk-7-1.png){width=672}\n:::\n:::\n\n\n\n## Exercise 4\n\nFit a model with **all** variables in the dataset as predictors and recreate the visualization above for this model.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_fit2 <- logistic_reg() |>\n  fit(spam ~ ., data = email)\n```\n\n::: {.cell-output .cell-output-stderr}\n\n```\nWarning: glm.fit: fitted probabilities numerically 0 or 1 occurred\n```\n\n\n:::\n\n```{.r .cell-code}\nlog_aug2 <- augment(log_fit2, email)\n\nggplot(log_aug2, aes(x = exclaim_mess, y = spam, color = .pred_class)) +\n  geom_jitter(alpha = 0.5)\n```\n\n::: {.cell-output-display}\n![](ae-14-spam-filter-A_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nlog_aug2 |>\n  count(spam, .pred_class) |>\n  group_by(spam) |>\n  mutate(p = n / sum(n))\n```\n\n::: {.cell-output .cell-output-stdout}\n\n```\n# A tibble: 4 × 4\n# Groups:   spam [2]\n  spam  .pred_class     n       p\n  <fct> <fct>       <int>   <dbl>\n1 0     0            3521 0.991  \n2 0     1              33 0.00929\n3 1     0             299 0.815  \n4 1     1              68 0.185  \n```\n\n\n:::\n:::\n",
    "supporting": [
      "ae-14-spam-filter-A_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}